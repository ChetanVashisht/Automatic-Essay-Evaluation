\documentclass[BTech]{nitkdiss}

\usepackage{amsmath,amssymb,graphicx,color,colortbl,grffile,float}

\title{Voice Conversion}
\author{Pulkit Pattnaik (12EC69)\\ Rajat Hebbar (12EC77)\\ Vinay Sundar Rajan (12EC101)\\}
\mentor{Dr. Deepu Vijayasenan}
\date{\today}
\department{Electronics and Communication Engineering}

\begin{document}

\maketitle
\newpage
\singlespacing
\begin{center}
\section*{DECLARATION}
\end{center}
\\
\\
\hspace{1cm}   We hereby declare that the Project Work Report entitled \textbf{"Voice Conversion"} which is being submitted to the National Institute of Technology Karnataka, Surathkal for the award of the Degree of \textbf{Bachelor of Technology in Electronics and Communication Engineering} is a bonafide report of the work carried out by us. The material contained in this Project Work Report has not been submitted to any University or Institution for the award of any degree
\\
\\
\\

\begin{center}


\begin{tabular}{ccc}

\hline
Name of the Student & Register No. & Signature with Date 			\\
\hline
Pulkit Pattnaik			&12EC69		&							\\\\
Rajat Hebbar			&12EC77		&							\\\\
Vinay Sundar Rajan		&12EC101		&							\\\\
\hline
\end{tabular}
\end{center}
\\
\\
\\
\\
Department of \textbf{Electronics and Communication Engineering}
\\
\\
Place : National Institute of Technology Karnataka, Surathkal
\\
Date : April 2016


\pagebreak
\begin{center}
\section*{Certificate}
\end{center}
This is to certify that the project entitled \textbf{"Voice Conversion"} submitted by : \\
\begin{center}
(1) Pulkit Pattnaik (12EC69),\\
(2) Rajat Hebbar (12EC77),\\
(3) Vinay Sundar Rajan (12EC101)
\end{center}
as the record of the work carried out by them, is \textit{accepted as the B. Tech Project Work Report Submission} in partial fulfilment of the requirements for the award of the degree of \textbf{Bachelor of Technology in Electronics and Communication Engineering}.\\ \\  
Guide
\\
\\
\\
Dr. Deepu Vijayasenan\\
Assistant Professor,\\
Department of Electronics and Communication Engineering.\\
National Institute of Technology Karnataka, Surathkal
\\
\\
\\
\\
Chairman-DUGC\\
(Signature with Date and Seal)
\newpage
\pagebreak
\section*{DEPARTMENT OF ELECTRONICS AND COMMUNICATION ENGINEERING}

\subsection*{Major Project -II}  
\subsubsection*{End Semester Evaluation Report, April 2016}

\textbf{Course  code :} EC498  \\
\textbf{Course Title:} Major Project -II \\
\textbf{Title:} \emph{Voice Conversion}
\subsubsection*{Project Group:}
\begin{tabular}{lcl}
\hline
Name of the Student & Register No. & Signature with Date 			\\
\hline
Pulkit Pattnaik			&12EC69		&							\\\\
Rajat Hebbar			&12EC77		&							\\\\
Vinay Sundar Rajan			&12EC101		&							\\\\
\hline
\end{tabular} \\\\\\\\\\
Place: NITK Surathkal\\
Date: 18-04-2016\hfill Name and Signature of Project Guide

\pagebreak
\begin{center}
\section*{ACKNOWLEDGEMENTS}
\end{center}
\\
We would like to thank Dr. Deepu Vijayasenan, for all his support throughout the project. We thank our examiner Dr. M.R. Arulalan for his support throughout the project. We also thank all the Department of Electronics and Communication Engineering, National Institute of Technology Karnataka, Surathkal for their support.
\newpage

\begin{abstract}
Voice conversion is an emergent problem in voice and speech processing with increasing commercial interest. The main aim of the  voice conversion system is to modify the speaker specific characteristics with respect to target specific characteristics. In this project we aim to extract suitable representative features from both source and target speakers and subsequently convert these features from the source to that of the target using various learning approaches. Performance evaluation of these learning models will then be conducted using objective and subjective features. Although majority of the work has been concentrated on learning text-dependent models, recently the focus has shifted to generic text-independent models.

Potential applications of voice conversion systems include security related usage (e.g. hiding the identity of the speaker), vocal pathology, voice restoration, as well as games and other entertainment applications, voice dubbing, language translation and personification of synthesized speech for automated systems in computer interaction tools, TTS(text to speech), etc. 
\end{abstract}



\newpage
\tableofcontents
\listoffigures
%\listoftables
\newpage
\pagestyle{plain}
\pagenumbering{arabic}

\chapter{\vspace{10px} Introduction}
\section{ Problem definition } 
A conventional problem in voice conversion involves a source speaker A whose speech characteristics are to be transformed in such a way that it closely resembles the speaker characteristics of the target speaker B. For designing a voice conversion model, there has to be a certain amount of training data available from both the source speaker and the target speaker. The training data can be either parallel or non-parallel training. The sentences spoken by the speakers can be known or unknown, corresponding to the cases of text dependent and text independent voice conversion.

Our goal is to achieve text independent voice conversion in the English language, which would demand a more generic model obtained from a suitably and sufficiently large data set. The data set includes sentences uttered by both the source and target speakers in an anechoic room with minimal noise, the utterances by both being as synchronized in time as possible. The speech samples are to be sourced from the project team members themselves. The sentences chosen encompass statistically common triphones to enable the constructed model to be adequately text independent.

Our secondary objective is to compare and contrast a few of the popularly used methods and algorithms for both feature extraction as well as model training and conclude upon their viabilities based on their pros and cons. A few of the feature extraction methods that have been targeted include Harmonic Noise Model (HNM), Linear Predictive Coding (LPC) Coefficients, Line Spectral Frequencies (LSF's), Mel Frequency Cepstral Coefficients (MFCC's) and STRAIGHT Spectra[9]. The training methods and models targeted are Gaussian Mixture Models (GMM's), Artificial Neural Networks (ANN's)[2] and Recurrent Temporal Restricted Boltzmann Machines (RTRBM's)[12]. 

\section{Previous work}

Despite the increased research attention that the topic has attracted, voice conversion has remained a challenging area. One of the challenges is that the perception of the quality and the success of the identity conversion are largely subjective. Furthermore, there is no unique correct conversion result: when a speaker utters a given sentence multiple times, each repetition is different. Due to these reasons, time-consuming listening tests must be used in the development and evaluation of voice conversion systems.

The use of linear prediction, and in particular the line spectral frequency (LSF) representation has been highly popular in Voice Conversion (VC) research [1],[3],[8],[11],[13], due to its favorable interpolation properties and the close relationship to the formant structure. In addition to the linear prediction based methods, cepstrum-based parametrization has been widely used, for example in the form of Mel-frequency cepstrum coefficients (MFCCs) [10].

The vast majority of the existing voice conversion systems deal with the conversion of spectral features. The most popular voice conversion approach has been Gaussian mixture model (GMM) based conversion [5],[10]. The data is modeled using a GMM and converted by a function that is a weighted sum of local regression functions. A GMM can be trained to model the density of source features only [10] or the joint density of both source and target features [5].

Artificial neural networks offer a powerful tool for modeling complex (nonlinear) relationships between input and output. They have been applied to voice conversion for example in [2]. The main disadvantage is the requirement of massive tuning when selecting the best architecture for the network.

The control of model complexity is a crucial issue when learning a model from data. There is a trade-off between two objectives: model fidelity and the generalization-capability of the model for unseen data. This trade-off problem, also referred to as bias-variance dilemma [4], is common for all model fitting tasks. In essence, simple models are subject to oversmoothing, whereas the use of complex models may result in overfitting and thus in poor prediction ability on new data.

\section{Motivation}
The fundamental motivating factor for this project is its relevance to modern day research on speech processing and recognition through Digital Signal Processing and Machine Learning. As aforementioned, in spite of increased research attention, the area of Voice Conversion has remained a challenging one. In particular, text independent VC techniques have so far been demandingly difficult, enough to elicit the experimentation of various algorithms and techniques. In an informal context, our motivation buds from the amusing prospect of being able to appreciate the speech sourced from one individual being voiced by another. 

\section{Overview}          

The project is currently being implemented in MATLAB R2013a. The initial phase of the project would involve collection of speech samples required for model training and creation of the speech database. Since our focus is on rendering text independent VC, the required data set would naturally have to be generic, i.e. sufficiently acquainted with utterances of English sentences including statistically common and frequently occurring triphones. Also, the recordings have to be, to the highest possible extent, free of echo, have ample SNR and be time synchronous between the two speakers. Currently the project is intended to be focused on male to male VC, since it is convenient for us to acquire male voice samples.

The second phase involves extraction of prosodic features from the acquired data set, possibly through all the various mentioned techniques one at a time for possible comparison and performance evaluation of the model. For every feature extraction technique chosen, the basic implementation methodology remains the same, as shown in the figure. [Include figure]. This is followed by a stage of post-processing of the extracted features including alignment.

The third phase would involve learning of the conversion models using the extracted features through any or all of the learning techniques mentioned. Our main focus lies on GMM's and Regression Models. During training phase, an estimated 70\% of the database would be used to train the models and the rest for testing.

The fourth and final phase involves testing of the trained models using the data from the acquired recordings themselves, and the evaluation of performance of the conversion method would be done through both objective and subjective analysis. Objective analysis includes calculation of Root Mean Square Error (RMSE), Karl Pearson's correlation coefficients, and Scatter plots for GMM based models. Subjective evaluation includes Mean Opinion Score (MOS) test subjected to various listeners. All this evaluation can finally be compiled into a comparative overview of the scenario of Voice Conversion techniques.

\begin{figure}[H]
\centering
\includegraphics[width=270px,height=350px]{Flow.PNG}
\caption{General Voice Conversion procedure}
\end{figure}

\chapter{\vspace{10px} Description}

\section{Feature Extraction}

Following suitable and necessary pre-processing stages of the speech samples such as pre-emphasis, voice activity detection and silence removal[7] and normalization, the first feature extraction method chosen was through Mel Frequency Cepstral Coefficients (MFCC's).

\begin{figure}[H]
\centering
\includegraphics[width=450px,height=90px]{MFCC Flow.PNG}
\caption{MFCC flow diagram}
\end{figure}

MFCC's are a compressed representation of spectral features varying in time. They are useful for associating spectral dependencies on temporal variations in a concise manner. The standard vector set of 13 cepstral coefficients combined with their associated delta and double-delta features gets us a 36 to 39 dimensional MFCC vector for each time frame of suitable size. Since from every chosen feature extraction method we aim to suitably be able to reconstruct the signal to as closely resemble the original signal as possible, the reconstruction using MFCC's was implemented in this phase itself to check its viability.

\noindent{\begin{minipage}{0.48\linewidth}
\begin{block}{}
\centering
\includegraphics[scale = 0.40]{Original Spectrogram.png}
\captionof{figure}{Original spectrogram}
\end{block}
\end{minipage}}
\hfill
\noindent{\begin{minipage}{0.48\linewidth}
\begin{block}{}
\centering
\includegraphics[scale = 0.40]{Reconstructed Spectrogram.png}
\captionof{figure}{Reconstructed spectrogram}
\end{block}
\end{minipage}}

Although MFCC's provided a very compact representation of the prosodic data, reconstruction of the original signal from the same proved to be extremely noisy due to the inherent noise of the reconstruction method. The current situation stands at finding a suitable solution for noise removal and improving the quality of reconstruction.

\section{Introduction to Harmonic Noise Model}

In order to obtain a high-quality speech synthesis we should take into the account that the speech signal for the voiced frames, is composed of two distinct components : a component which reflects the periodicity of the speech signal and the other for the friction noise, turbulences of the glottal airflow,etc. The speech models which make this kind of decomposition of the speech signal are often referred to as hybrid or sinusoidal + noise models. If now the frequencies of the sinusoids are restricted to be multiples of a fundamental frequency the models are called harmonic+noise models. The proposed HNM assumes the speech signal to be composed of a harmonic part $h(t)$ and noise part $n(t)$. For a voiced speech signal, the spectrum is divided into two bands delimited by the so called maximum voiced frequency $F_{m}(t)$, a time varying parameter. The lower band of the spectrum below the maximum voiced frequency is represented by the harmonic part, while the upper band by the noise part. Thus the harmonic part accounts for the periodic structure of the speech signal and this part designates sums of harmonically related sinusoidal components with continuously time varying amplitudes and phases.$$h(t)=\sum_{k=1}^{K(t)}a_{k}(t)cos(\phi_{k}(t))$$ where $a_{k}(t)$ and $\phi_{k}(t)$ are respectively the amplitude and phase at time $t$ of the $k^{th}$ superscript. Note that $K(t)$ represents the time varying number of pitch harmonics included in the harmonic part. These parameters are updated at specific time instants $t_{i}$. The time interval between two successive time instants $t_{i}$ and $t_{i+1}$ is a frame. The derivative of the phase is defined as the instantaneous frequency which in HNM is a time varying harmonic frequency $2\pi kf_{0}(t)$. 

The noise part accounts for the unvoiced frames and for the friction noise. For voiced frames, the noise part is a high pass signal, This high pass signal exhibits a specific frequency content as well as a time domain structure in energy localization.

\section{Estimation of HNM parameters}
The first step of the analysis process consists of estimating the fundamental frequency and the maximum voiced frequency for the voiced frames. The first step of the algorithm consists of an initial estimation of the pitch based on an auto-correlation domain approach. The initial pitch estimation is used to detect the voicing, the maximum voicing frequency and finally to refine the initial pitch. All these detections are made in the frequency domain.

\subsection{Initial Pitch Estimation}
The initial pitch estimation has to be based on a criterion which takes into account how close the synthesized speech will be to the original speech. The normalized criterion used is $$E=\dfrac{\sum_{t=-\infty}^{\infty}s^{2}(t)w^{2}(t)-P.\sum_{l=-\infty}^{\infty}r(l.P)}{[\sum_{t=-\infty}^{\infty}s^{2}(t)w^{2}(t)][1-P.\sum_{t=-\infty}^{\infty}w^{4}(t)]}$$ where $s(t)$ is the speech signal, $w(t)$ is the analysis window which is subject to the constraint $$\sum_{t=-\infty}^{\infty}w^{2}(t)=1$$ The function $r(k)$ is defined as $$r(k)=\sum_{t=-\infty}^{\infty}s(t)w^{2}(t)s(t+k)w^{2}(t+k)$$ The above function is evaluated for periods $P$ in the set $[\dfrac{f_{s}}{f_{0max}},....,\dfrac{f_{s}}{f_{0min}}]$, where the $f_{0min}$ and $f_{0max}$ are the minimum and maximum searched fundamental frequencies. Typical range of values of minimum and maximum frequencies are $60 - 230 Hz$ for a male voice and $180 - 400 Hz$ for a female voice.

\begin{figure}[H]
\centering
\includegraphics[width=300px,height=270px]{Pitch Analysis.png}
\caption{Pitch Analysis flow}
\label{Figure 1}
\end{figure}

\subsection{Voiced/Unvoiced Decision}

Using the initial fundamental frequency, we generate a synthetic signal $\hat{s}(t)$ as the sum of harmonically related sinusoids with amplitudes and phases estimated by the DFT algorithm. Denoted as $\lvert \tilde{S}(f)\rvert$ the synthetic spectrum and $\lvert S(f)\rvert$ the original spectrum, the voiced/unvoiced decision is made by comparing the normalized error over the first four harmonics of the estimated fundamental frequency to a given threshold $$E=\dfrac{\int_{0.7\hat{f_{0}}}^{4.3\hat{f_{0}}}(\lvert S(f)\rvert-\lvert \tilde{S}(f)\rvert)^2}{\int_{0.7\hat{f_{0}}}^{4.3\hat{f_{0}}}(\lvert S(f)\rvert)^2}$$ where $\hat{f}_{0}$ is the initial fundamental frequency. If the error $E$ is below the threshold this frame is marked as voiced. Otherwise, as unvoiced.

\subsection{Maximum Voiced Frequency and Pitch Refining}
 The maximum voiced frequency is the maximum frequency considered as a valid harmonic in a voiced frame. This is required to demarcate the high-frequency noise boundary which would have to be synthesized separately. Since, the method to determine MVF as proposed in the thesis did not yield satisfactory results, a simple threshold-based MVF estimation was implemented with the assumption, in general, voiced frequencies occur within $5kHz$ . The mean of the spectrum within $5 kHz$ was used as a decision parameter. This algorithm yielded MVFs in the frequency range $3-5 kHz$.
The estimated pitch has to be refined, since it is susceptible to quantization error due to FFT. Using the initial pitch estimation $f_{0}$ and the frequencies $f_{i}$ classified as voiced from the previous step, the refined pitch, $\hat{f}_{0}$, is defined as the value which minimizes the error: $$E(\hat{f}_{0})=\sum_{i=1}^{L_{n}}\lvert f_{i}-i.\hat{f}_{0} \rvert^{2}$$ where $L_{n}$ is the number of the voiced frequencies $f_{i}$.


\subsection {Amplitude and Phase Estimation}
Using the stream of pitch values estimated by the above procedure, the analysis time-instants, $t_{a}^{i}$, are then set at a pitch synchronous rate on the voiced portions of speech, and at a fixed rate($10ms$) on unvoiced segments $$t_{a}^{i+1} = t_{a}^{i} + P(t_{a}^{i})$$ where $P(t_{a}^{i})$ is the pitch period at time-instant $t_{a}^{i}$.

\subsubsection{Amplitude Estimation}
It can be assumed that the amplitudes of the harmonics and the pitch period are nearly constant around the analysis time-instant $t_{a}^{i}$. Assuming that the interaction among the harmonics is insignificant, the complex amplitudes of the $k_{th}$ exponential is given by $$A_{k} = \dfrac {\sum_{t=t_{a}^{i}-N}^{t_{a}^{i}+N}w^{2}(t)s(t)e^{-j2\pi kf_{0}t}}{\sum_{t=t_{a}^{i}-N}^{t_{a}^{i}+N}w^{2}(t)}$$

\subsubsection{Phase Estimation}
The phase envelope is obtained as follows. At the first voiced frame of a voiced portion of signal, the phase is unwrapped in the frequency domain by adding integer multiples of $2\pi$ in order to keep the variation of the phase slope $d\phi_{k}$ as smooth as possible. The phase slope d$\phi_{k}$ being defined as $$d\phi_{k} = \phi_{k+1} - \phi_{k}$$ where k denotes the $k^{th}$ harmonic. 
In the next voiced frame, the phases are unwrapped by using the phase slopes from the previous frame (and not the phase slopes of the current one as was the case for the first frame of the voiced portion).

\begin{figure}[H]
\centering
\includegraphics[width=300px,height=200px]{Phase Unwrapping.png}
\caption{Phase Unwrapping}
\end{figure}

\subsection{Noise part estimation}
The frequency contents of noise part is described by a time varying AR envelope, its time domain structure is represented by a piecewise linear energy-envelope function.
The noise part $n(t)$ is therefore obtained by filtering a white Gaussian noise $u(t)$ by  time-varying, normalized all pole filter $h(t,\tau)$ and multiplying the result by an energy envelope function $e(t)$ $$n(t)=e(t)[h(t,\tau)\ast u(t)]$$ The filter $h(t,\tau)$ is evaluated for each time instant $t_{i}$.

In each analysis frame, the spectral density of the original signal is modeled by a $p^{th}$-order all-pole filter, $H(t_{i},z)$ by use of a standard correlation based method. Also over the same duration, the variance of the original signal is estimated, representing the gain of the filter. For the signal sampled the order of the AR model is set to $15$.

\begin{figure}[H]
\centering
\includegraphics[width=250px,height=200px]{Energy envelope.png}
\caption{Energy envelope of noise}
\end{figure}

\section{Synthesis}

\subsection{Harmonic Synthesis}
Let $a_{k}^{i}$, $\phi_{k}^{i}$, $f_{0}^{i}$ and $a_{k}^{i+1}$, $\phi_{k}^{i+1}$, $f_{0}^{i+1}$ denote the sets of parameters at synthesis time instants $t_{s}^{i}$ and $t_{s}^{i+1}$ for the $k^{th}$ harmonic respectively. The amplitudes and phases are obtained by sampling the phase and spectral envelope at the harmonics of the fundamental frequencies $f_{0}^{i+1}$ and $f_{0}^{i+1}$. The instantaneous amplitude $a_{k}(t)$ is obtained by linear interpolation of the estimated amplitudes at the frame boundaries $$a_{k}^{t}=a_{k}^{i} + \dfrac{a_{k}^{i+1}-a_{k}^{i}}{t_{s}^{i+1}-t_{s}^{i}}t$$ for $t_{s}^{i}<=t<t_{s}^{i+1}$. Similarly the instantaneous frequency $f_{0}(t)$ and instantaneous phase $\phi_{k}(t)$ are also obtained by similar linear interpolation of the estimated values at the frame boundaries.

Having determined the instantaneous values of the harmonic amplitude and phases the final harmonic part will be given by $$\hat{h}(t)=\sum_{k=0}^{L}a_{k}(t)cos(2\pi k(\dfrac{f_{0}}{f_{s}})t+\phi_{k}(t))$$

\begin{figure}[H]
\centering
\includegraphics[width=400px,height=230px]{Block diagram.png}
\caption{Block diagram of synthesis of harmonic part}
\end{figure}

\subsection{Noise Synthesis}
 For each analysis time instant $t_{a}^{i}$ a signal is synthesized by filtering a unit variance white Gaussian noise through a normalized lattice filter. The length of the noise is equal to twice the local pitch period. In the context of autoregressive signal modeling the coefficients of the lattice filter (K-parameters) are called reflection coefficients or PARCOR coefficients. Given the polynomial coefficients $\alpha$ of the $p^{th}$ order all-pole filter estimated at the time $t_{a}^{i}$ the K-parameters are obtained by the recurrence formula $$k_{i}=\alpha_{i}^{(i)}$$ $$\alpha_{m}^{(i-1)}=\dfrac{\alpha_{m}^{(i)}-k_{i}\alpha_{i-m}^{(i)}}{1-k_{i}^{2}}$$ where $m=1,2,...,(i-1)$ for $i=p,p-1,...,1$.
The output of the normalized lattice filter is multiplied by the estimated variance at the analysis time instant $t_{a}^{(i)}$. Hence the synthesized noise signal at this point of the synthesis step has the same variance as the original signal and covers the whole frequency range. However in the frame to synthesize this voice the frequencies from 0 to the maximum voiced frequency have been synthesized by the harmonic part. Thus and only if the frame is voiced, the synthesized noise signal is filtered by a high pass filter with cutoff frequency equal to the local MVF. Obviously if the frame is unvoiced the harmonic part is 0 and the above step of high pass filtering is omitted. Denoted now by $n(t,t_{s}^{i})$, the synthesized noise signal at $t_{s}^{i}$ and by $n(t,t_{s}^{i-1})$ the noise signal synthesized at the previous synthesis time instant  $t_{s}^{i-1}$, the noise part for the current frame is obtained by overlapping and adding $n(t,t_{s}^{i})$ with $n(t,t_{s}^{i-1})$. Finally and also only if the frame to be synthesized is voiced, the time domain envelope is directly applied to the noise part.

\begin{figure}[H]
\centering
\includegraphics[width=400px,height=250px]{Noise synthesis flow.png}
\caption{Noise synthesis flow}
\end{figure}

\section{Amplitude envelope estimation - Discrete Regularized Cepstrum}
The amplitude envelope is computed from the estimated amplitudes of the harmonics by a discrete regularized cepstrum method using a warped frequency scale. Given a set of L values of harmonic amplitudes $a_{k}$ measured at the normalized harmonic frequencies of the fundamental frequency $f_{0}$, $f_{k} = kf_{0}/f_{s}$ where $f_{s}$ is a sampling frequency, the discrete cepstrum is obtained by minimizing the following squared error in the log spectral domain. $$\epsilon=\sum_{k=1}^{L}{\lvert \lvert log a_{k} - log\lvert S(f_{k};c)\rvert \rvert \rvert}^{2}$$ The spectrum magnitude $\lvert S(f;c)\rvert$ is related to the real cepstrum coefficients by $$log\lvert S(f;c)\rvert=c_{0}+2\sum_{i=1}^{p}c_{i}cos(2\pi fi)$$ where $c=[c_{0},...,c_{p}]^{T}$ are the real cepstrum coefficients, and p is the order of the cepstrum. The least squares solution is $$c=(M^{T}M)^{-1}M^{T}a$$ where $a=[log(a_{1})...log(a_{L})]$ are the specified log magnitude values, and matrix M is defined as $$M=\begin{bmatrix}1&2cos(2\pi f_{1})&2cos(2\pi f_{1}2)&\dots&2cos(2\pi f_{1}p)\\ \vdots&\vdots&\vdots&\vdots\\ 1&2cos(2\pi f_{L})&2cos(2\pi f_{L}2)&\dots&2cos(2\pi f_{L}p)\end{bmatrix}$$ The problem with this solution is that the matrix $M^{T}M$ is in general ill-conditioned when p approaches L(singular when $p\geqslant L$). As the number L is a time variable parameter(L is the number of harmonics included int the harmonic part), the order p must be chosen as small as possible in order to avoid the ill-conditioned problem. However, very small orders lead to not a good fit at the specified frequency points. Regularization techniques are well known for obtaining well behaved solutions to over parametrized estimation problems. Applying this regularization the solution can now be shown to be $$c=[M^{T}M+\lambda R]^{-1}M^{T}a$$

\section{Time Alignment of speech signals}
Dynamic Time Warping(DTW) is a time series alignment algorithm aimed at aligning two sequences of feature vectors by warping the time axis iteratively until an optimal match between the two sequences is found. Consider two sequences of feature vectors : $$A = a_{1},a_{2},...,a_{i},...,a_{n}$$ $$ B = b_{1},b_{2},...,b_{j},...,b_{m}$$ The two sequences can be arranged on the sides of a grid, with one on the top and the other up the left hand side. Both sequences start on the bottom left of the grid. Inside each cell a distance measure can be placed, comparing the corresponding elements of the two sequences. To find the best match or alignment between these two sequences one needs to find a path through the grid which minimizes the total distance between them. The procedure for computing this overall distance involves finding all possible routes through the grid and for each one compute the overall distance. The overall distance is the minimum of the sum of distances between the individual elements on the path divided by the sum of the weighting function. The weighting function is used to normalise for the path length. The major optimisations or constraints of the DTW algorithm arise from the observations on the nature of acceptable paths through the grid:\\
\textbf{Monotonic condition}: the path will not turn back on itself, both the i and j indexes either stay the same or increase, they never decrease.\\
\textbf{Continuity condition}: the path advances one step at a time. Both i and  can only increase by at most 1 on each step along the path. \\
\textbf{Boundary condition}: the path starts at the bottom left and ends at the top right.\\
Warping window condition: a good path is unlikely to wander very far from the diagonal. The distance that the path is allowed to wander is the window width. \\
\textbf{Slope constraint condition}: The path should not be too steep or too shallow. This prevents short sequences matching too long ones. The condition is expressed as a ratio p/q where p is the number of steps allowed in the same(horizontal or vertical) direction. After p steps in the same direction is not allowed to step further in the same direction before stepping atleast q times in the diagonal direction.\\
The power of the DTW algorithm is in the fact that instead of finding all possible routes through the grid which satisfy the above conditions, the DTW algorithm works by keeping track of the cost of the best path to each point in the grid. During the calculation process of the DTW grid, it is not known which path is minimum overall distance path, but this can be traced back when the end point is reached.

\section{Dataset Preparation}
A pool of $70$ phonetically balanced sentences obtained from the list of Harvard sentences were recorded by two speakers. $5$ utterances per speaker were recorded, resulting in a total of $350$ utterances, equivalent to around $12$ minutes of data per speaker. These $5$ utterances were recorded to account for the variation in speech uttered by a single speaker.

\section{Feature Vector}
After the HNM modeling of the each utterance, three feature vectors are created per frame of the speech : \begin{enumerate}
\item Pitch Vector - a $2$-dimensional vector constructed by concatenating the corresponding pitch values of the two speakers. 
\item Cepstral Vector - a $52$-dimensional vector constructed by concatenating the $26$-cepstral coefficients of each individual speaker. 
\item Noise Vector - a $32$-dimensional vector consisting of $15$ autoregressive coefficients and $1$ variance modeling the noise of each speaker concatenated together.
\end{enumerate}
These three feature vectors are trained separately in the training stage. The three feature sets were trained separately as three different Gaussian Mixture models since it was observed that such separate model training and testing proved to be give more accurate conversion results than when the features trained were concatenated together and trained as a single GMM.

\section{Training}
Here GMM training is used to model the conversion function. To implement the GMM, HMM Toolkit(HTK) is used. The actual training process takes place in stages. Firstly, an initial set of models must be created. On the first cycle, the training data is uniformly segmented, each model state is matched with the corresponding data segments and then means and variances are estimated. If mixture Gaussian models are being trained, then a modified form of k-means clustering is used. On the second and successive cycles, the uniform segmentation is replaced by Viterbi alignment. The initial parameter values computed by HInit are then further re-estimated by HRest. Again, the fully labelled bootstrap data is used but this time the segmental k-means procedure is replaced by the Baum-Welch re-estimation procedure described in the previous chapter. When no bootstrap data is available, a so-called flat start can be used. In this case all of the phone models are initialised to be identical and have state means and variances equal to the global speech mean and variance. The tool HCompV can be used for this.\bigskip\\
Once an initial set of models has been created, the tool HERest is used to perform embedded training using the entire training set. HERest performs a single Baum-Welch re-estimation of the whole set of HMM phone models simultaneously. HERest is the core HTK training tool. It is designed to process large databases, it has facilities for pruning to reduce computation and it can be run in parallel across a network of machines.\bigskip\\
The philosophy of system construction in HTK is that HMMs should be refined incrementally. Thus, a typical progression is to start with a simple set of single Gaussian context-independent phone models and then iteratively refine them by expanding them to include context-dependency and use multiple mixture component Gaussian distributions. The tool HHEd is a HMM definition editor which will clone models into context-dependent sets, apply a variety of parameter tyings and increment the number of mixture components in specified distributions. The usual process is to modify a set of HMMs in stages using HHEd and then re-estimate the parameters of the modified set using HERest after each stage.\bigskip\\
The number of gaussians used in the mixture for pitch was $32$, while for noise it was $16$ and for cepstrum it was $32$ based on the amount of collected training data.

\section{Testing}

The generated voice conversion model corresponding to these two speakers is tested using any test sample from the source speaker. The target speech vector obtained is used to reconstruct the speech in the target speaker's voice. Since this is a vector quantization by using Gaussian mixtures, the source vector is tested against each of the generated gaussians to find the best match. The target vector then is approximated by the means of that matched multivariate gaussian.\bigskip\\
The target vectors obtained from three separate tests corresponding to pitch, harmonics and noise are then used to reconstruct the target voice sample.

\section{Reconstruction of target voice}

The target voice is reconstructed from the converted features in two main steps. First, the harmonic part is reconstructed using the converted  cepstral coefficients and pitch data. The corresponding spectrum of each time frame is generated using the cepstral coefficients, and this spectrum consists of the fundamental frequency and the required number of harmonics. Then the time domain samples of each frame are synthesized from the spectral data by overlap add synthesis. This corresponds mainly to the Voiced frames without noise.\bigskip\\
Second, the target noise is generated using the converted Arburg coefficients and variances for each frame. These synthesized time domain samples of Harmonic and Noise data of the target speech are then added together to obtain the Final Converted Speech.

\chapter{\vspace{10px} Results}

The analysis and synthesis procedure was carried out for various voiced samples consisting of standard sentences. The sample selected as an example here is ``Glue the sheet to the dark blue background by a male voice''.

\begin{figure}[H]
\centering
\includegraphics[width=400px,height=230px]{Special Autocorrelation.png}
\caption{Special Autocorrelation estimate}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=400px,height=230px]{Pitch Tracking of voiced frames.png}
\caption{Pitch Tracking for the sample ``Glue the sheet to the dark blue background''}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=400px,height=230px]{Praat Result.png}
\caption{Praat result for pitch tracking of the same sample}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=400px,height=230px]{VUD.png}
\caption{Voiced/Unvoiced Decision}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=400px,height=230px]{MVF.png}
\caption{Maximum  Voiced Frequency estimation}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=400px,height=300px]{Spectral Envelope.png}
\caption{Spectral envelope}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=450px,height=250px]{Phase unwrapping result.png}
\caption{Phase Unwrapping Result}
\end{figure}

\textbf{Original and synthesized speech spectrograms for male voice} \\
\noindent{\begin{minipage}{0.48\linewidth}
\centering
\begin{block}{}
\includegraphics[width=160px,height=170px]{Original Speech Spectrogram.jpg}
\captionof{figure}{Original Speech Spectrogram}
\end{block}
\end{minipage}}
\hfill
\noindent{\begin{minipage}{0.48\linewidth}
\begin{block}{}
\includegraphics[width=160px,height=170px]{Synthesized Speech Spectrogram.jpg}
\captionof{figure}{Synthesized Spectrogram}
\end{block}
\end{minipage}

\textbf{Original and synthesized speech spectrograms for female voice} \\ 
\bigskip
\noindent{\begin{minipage}{0.48\linewidth}
\centering
\begin{block}{}
\includegraphics[width=160px,height=170px]{Female_Original - Its easy to tell the depth of a well.jpg}
\captionof{figure}{Original Speech Spectrogram}
\end{block}
\end{minipage}}
\hfill
\noindent{\begin{minipage}{0.48\linewidth}
\begin{block}{}
\includegraphics[width=160px,height=170px]{Female_Synthesized - Its easy to tell the depth of a well.jpg}
\captionof{figure}{Synthesized Spectrogram}
\end{block}
\end{minipage}

\textbf{Voiced and unvoiced components of speech} \\
\bigskip
\bigskip
\noindent{\begin{minipage}{0.48\linewidth}
\begin{block}{}
\includegraphics[width=200px,height=150px]{Voiced Speech.jpg}
\captionof{figure}{Voiced speech}
\end{block}
\end{minipage}}
\hfill
\noindent{\begin{minipage}{0.48\linewidth}
\begin{block}{}
\includegraphics[width=200px,height=150px]{Unvoiced Speech.jpg}
\captionof{figure}{Unvoiced speech}
\end{block}
\end{minipage}}
\noindent{\begin{minipage}{0.48\linewidth}
\begin{block}{}
\includegraphics[width=200px,height=150px]{Voiced Speech Spectrogram.jpg}
\captionof{figure}{Voiced spectrogram}
\end{block}
\end{minipage}}
\hfill
\noindent{\begin{minipage}{0.48\linewidth}
\begin{block}{}
\includegraphics[width=200px,height=150px]{Unvoiced Speech Spectrogram.jpg}
\captionof{figure}{Unvoiced spectrogram}
\end{block}
\end{minipage}}

\noindent{\begin{minipage}{0.48\linewidth}
\begin{block}{}
\includegraphics[scale = 0.3]{15th Order Cepstral Fit.jpg}
\captionof{figure}{15th Order Cepstral Fit}
\end{block}
\end{minipage}}
\hfill
\noindent{\begin{minipage}{0.48\linewidth}
\begin{block}{}
\includegraphics[scale = 0.3]{30th Order Cepstral Fit.jpg}
\captionof{figure}{30th Order Cepstral Fit}
\end{block}
\end{minipage}}

\newpage
\textbf{DTW Distance Plot} \\
\bigskip
\begin{figure}[H]
\centering
\includegraphics[width=280px,height=200px]{Distance Plot.jpg}
\caption{Distance Plot}
\end{figure}

\textbf{Pitch Conversion}\\
\bigskip

\noindent{\begin{minipage}{0.48\linewidth}
\begin{block}{}
\includegraphics[scale = 0.25]{Pitch Conversion.jpg}
\captionof{figure}{Converted Pitch}
\end{block}
\end{minipage}}
\hfill
\noindent{\begin{minipage}{0.48\linewidth}
\begin{block}{}
\includegraphics[scale = 0.25]{Original Target Pitch.jpg}
\captionof{figure}{Original Target Speaker Pitch}
\end{block}
\end{minipage}}

\newpage
\textbf{Source and Target Spectrograms}\\
\bigskip
\bigskip
\noindent{\begin{minipage}{0.48\linewidth}
\begin{block}{}
\includegraphics[scale = 0.3]{Source Spectrogram.jpg}
\captionof{figure}{Source Spectrogram}
\end{block}
\end{minipage}}
\hfill
\noindent{\begin{minipage}{0.48\linewidth}
\begin{block}{}
\includegraphics[scale = 0.31]{Target Spectrogram.jpg}
\captionof{figure}{Target Spectrogram}
\end{block}
\end{minipage}}

\chapter{\vspace{10px} Conclusion}
As mentioned in the earlier sections, HNM modeling of speech is divided into a number of stages. We have been able to accurately estimate pitch which we have tested for multiple speech samples and compared our results with those generated by Praat. Our Voiced/Unvoiced decision has also produced satisfactory results which was verified using Praat. The method used for estimating Maximum Voiced Frequency was modified from the one proposed in the thesis. However we have been able to cross-check these values from the FFT. Amplitude estimation was implemented using a localized windowed FFT and it gave considerably accurate results. As it was observed that phase continuity had to be essentially satisfied for fairly noiseless reconstruction of voiced speech, the phase values were synthesized and stitched between frames as opposed to our previous approach of importing unwrapped phase values from the original speech spectra. To avoid abrupt transition between time frames during synthesis, overlap add synthesis was adopted using suitably sized triangular windowed frames.\bigskip\\
It was essential for the Voiced/Unvoiced frames to be detected as accurately as possible since an error in this would result in cepstral fits which involve coefficients that vary too large when compared to those of proper Voiced frames. This would result in Gaussians fitting inaccurate values and the converted cepstral coefficients are observed to be completely wrong.\bigskip\\
It was observed from the GMM training phase that the training of independent GMM models for Pitch, Harmonics as well as Noise proved to give more accurate conversion results than when a single GMM model was trained using a concatenated feature vector consisting of all the three components. The number of gaussians chosen in the mixture was dependent on the amount of training data present as well as the required level of accuracy of that component.\bigskip\\
Although the converted speech seems fairly recognizable as that of the target, owing to the vector quantization, the converted speech has hints of discrete values which renders it slightly robotic. This may be improved upon by using a higher number of gaussians in the mixture model. Since noise played a major role in making the sample as natural sounding as possible, any approach to voice conversion adopted should involve fairly good modeling of noise. We have observed that this approach of Harmonic and Noise Modeling has a good approximation of noise both in terms of accuracy as well as the number of coefficients required to represent the noise as accurately as possible.\bigskip\\
Voice unvoiced decision thresholds used are still dependent on the speaker's gender. In order to have perfectly synchronized time frames from the two speakers, the dynamic time warping process has to be accurate.

\begin{thebibliography}{9}

\bibitem AArslan, L. (1999). Speaker transformation algorithm using segmental codebooks (STASC), Speech Communication 28(3): 211–226.

\bibitem KKevin D'souza, K.T.V. Talele, "Voice Conversion Using Gaussian Mixture Models", 2015 International Conference on Communication, Information & Computing Technology (ICCICT), Jan. 16-17, Mumbai, India.

\bibitem MM.H. Moattar, M.M. Homayounpour : A Simple but Real-Time Voice Activity Detection Algorithm. Laboratory for Intelligent Sound and Speech Processing (LISSP), Computer Engineering and Information Technology Dept., Amirkabir University of Technology, Tehran, Iran. 17 th European Signal Processing Conference (EUSIPCO), Glasgow, Scotland, August 2009.

\bibitem NNurminen, J., Popa, V., Tian, J., Tang, Y. & Kiss, I. (2006). A parametric approach for voice conversion, Proc. of TC-STAR Workshop on Speech-to-Speech Translation, pp. 225–229.

\bibitem RRyo Aihara, Ryoichi Takashima, Tetsuya Takiguchi (2012). GMM-Based Emotional Voice Conversion Using Spectrum and Prosody Features. American Journal of Signal Processing, 2(5): 134-138.

\bibitem SStylianou, Y., Cappe, O. & Moulines, E. (1998). Continuous probabilistic transform for voice conversion, IEEE Trans. Audio, Speech, Lang. Process. 6(2): 131–142.

\bibitem TTao, J., Zhang, M., Nurminen, J., Tian, J. & Wang, X. (2010). Supervisory data alignment for text-independent voice conversion, IEEE Trans. Audio, Speech, Lang. Process. 18(5): 932–943.

\bibitem TTurk, O. & Arslan, L. (2006). Robust processing techniques for voice conversion, Computer Speech and Language 4(20): 441–467.

\bibitem HHarvard Sentences - www.cs.columbia.edu/$\sim$ hgs/audio/harvard.html
\end{thebibliography}

\end{document}
